{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "927b937f-994e-4bc0-9a7b-aadd97dfb4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a shapes dataset. We will start extremely simple, we will have circles and squares, and they can be blue or red. We will have slots \n",
    "# attend to a shape and then predict a) what the shape is, and b) what the color is. Baseline is random guess for each. \n",
    "\n",
    "# 1. Make sure slot attention module is correct (based on both PyTorch and TF githubs) DONE\n",
    "# 2. Make sure encoder is correct (based on both PyTorch and TF githubs)\n",
    "\n",
    "# ... learn how to use PyTorch Lightning, try quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "17ff6053-fbb3-49c2-8075-dc8d8c0446e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "from typing import List \n",
    "import random\n",
    "import math \n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import torch.nn.init as init\n",
    "from typing import Callable, Optional, Tuple\n",
    "import os\n",
    "import h5py\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "19ac01ce-3b0f-4ff7-9546-3e54ec7ab267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeType(Enum):\n",
    "    CIRCLE = \"Circle\"\n",
    "    SQUARE = \"Square\"\n",
    "\n",
    "class ColorType(Enum):\n",
    "    RED = [255, 0, 0]\n",
    "    BLUE = [0, 0, 255]\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Shape: \n",
    "    shape_type: ShapeType\n",
    "    color: ColorType \n",
    "    center: List[int]\n",
    "    \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ImageData:\n",
    "    circle: Shape\n",
    "    square: Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "daa445a6-bea0-45f8-8a29-bdda9502152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "RADIUS = 5\n",
    "IMAGE_SIZE = 30\n",
    "DATASET_SIZE = 500\n",
    "assert 2*RADIUS < IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "7015c598-9a2c-45a5-bc3d-60626b933074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_shape_image(image_size: int, radius: int) -> np.ndarray:\n",
    "    \"\"\"Create random shape mask\"\"\"\n",
    "    assert 2*radius < image_size\n",
    "    \n",
    "    # Create a blank grey image (using 3 channels for RGB)\n",
    "    image = np.ones((image_size, image_size, 3), dtype=np.uint8) * 200  # Grey background\n",
    "\n",
    "    # Choose two random centers from a grid of possible values\n",
    "    grid_values = [i*RADIUS for i in range(1, IMAGE_SIZE//RADIUS)]\n",
    "    x1, y1, x2, y2 = random.sample(range(len(grid_values)), 4)\n",
    "    center_1 = [grid_values[x1], grid_values[y1]]\n",
    "    center_2 = [grid_values[x2], grid_values[y2]]\n",
    "\n",
    "    # Choose random colors\n",
    "    color_1 = COLORS[random.randint(0, len(COLORS) - 1)]\n",
    "    color_2 = COLORS[random.randint(0, len(COLORS) - 1)]\n",
    "    \n",
    "    # Create a grid of pixel coordinates\n",
    "    y, x = np.ogrid[:image_size, :image_size]  \n",
    "\n",
    "    # Construct circle mask \n",
    "    distance_from_center = (x - center_1[0])**2 + (y - center_1[1])**2  # Euclidean distance squared\n",
    "    mask = distance_from_center <= radius**2  # Circle condition\n",
    "    image[mask] = color_1.value\n",
    "\n",
    "    # Construct square maks\n",
    "    mask = (((x >= center_2[0] - radius) * (x <= center_2[0] + radius)) * ((y >= center_2[0] - radius) * (y <= center_2[0] + radius)))\n",
    "    image[mask] = color_2.value  \n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "9933f61f-2592-4e85-a8b2-5dac79fb5bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_two_shape_dataset(image_size: int, radius: int, dataset_size: int) -> List[ImageData]:\n",
    "    image_data = []\n",
    "\n",
    "    for _ in range(dataset_size): \n",
    "        # Choose two random centers from a grid of possible values\n",
    "        grid_values = [i*RADIUS for i in range(1, IMAGE_SIZE//RADIUS)]\n",
    "        x1, y1, x2, y2 = random.sample(range(len(grid_values)), 4)\n",
    "        center_1 = [grid_values[x1], grid_values[y1]]\n",
    "        center_2 = [grid_values[x2], grid_values[y2]]\n",
    "    \n",
    "        # Choose random colors\n",
    "        color_1, color_2 = random.choices([x for x in ColorType], k=2)\n",
    "    \n",
    "        circle = Shape(shape_type=ShapeType.CIRCLE, color=random.choice([x for x in ColorType]), center=center_1)\n",
    "        square = Shape(shape_type=ShapeType.SQUARE, color=random.choice([x for x in ColorType]), center=center_2)\n",
    "    \n",
    "        image_data.append(ImageData(circle=circle, square=square))\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "efa408c4-bca4-4804-9aca-4ae372197f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeDataset(Dataset):\n",
    "    def __init__(self, image_size: int, radius: int, dataset_size: int):\n",
    "        self.image_size = image_size\n",
    "        self.radius = radius\n",
    "        self.dataset_size = dataset_size\n",
    "        self.data = create_two_shape_dataset(self.image_size, self.radius, self.dataset_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single sample (features and label) by index\n",
    "        image_data = self.data[idx]\n",
    "\n",
    "        # Create a blank grey image (using 3 channels for RGB)\n",
    "        image = np.ones((self.image_size, self.image_size, 3), dtype=np.uint8) * 200  # Grey background\n",
    "        \n",
    "        # Create a grid of pixel coordinates\n",
    "        y, x = np.ogrid[:self.image_size, :self.image_size]  \n",
    "    \n",
    "        # Construct circle mask \n",
    "        distance_from_center = (x - image_data.circle.center[0])**2 + (y - image_data.circle.center[1])**2  # Euclidean distance squared\n",
    "        mask = distance_from_center <= self.radius**2  # Circle condition\n",
    "        image[mask] = image_data.circle.color.value\n",
    "    \n",
    "        # Construct square maks\n",
    "        mask = (((x >= image_data.square.center[0] - self.radius) * (x <= image_data.square.center[0] + self.radius)) * ((y >= image_data.square.center[0] - self.radius) * (y <= image_data.square.center[0] + self.radius)))\n",
    "        image[mask] = image_data.square.color.value  \n",
    "\n",
    "        # label = np.zeros(4)\n",
    "        # if image_data.circle.color.name == \"RED\" and image_data.square.color.name == \"RED\":\n",
    "        #     label[0] = 1\n",
    "        # elif image_data.circle.color.name == \"RED\" and image_data.square.color.name == \"BLUE\":\n",
    "        #     label[1] = 1\n",
    "        # elif image_data.circle.color.name == \"BLUE\" and image_data.square.color.name == \"RED\":\n",
    "        #     label[2] = 1\n",
    "        # elif image_data.circle.color.name == \"BLUE\" and image_data.square.color.name == \"BLUE\":\n",
    "        #     label[3] = 1\n",
    "\n",
    "        label = np.zeros(2)\n",
    "        if image_data.circle.color.name == \"RED\":\n",
    "            label[0] = 0\n",
    "        else:\n",
    "            label[0] = 1\n",
    "\n",
    "        if image_data.square.color.name == \"RED\":\n",
    "            label[1] = 0\n",
    "        else:\n",
    "            label[1] = 1\n",
    "\n",
    "\n",
    "        return torch.tensor(image, dtype=torch.float32).permute(2, 0, 1), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "12e6a553-7117-4312-b878-e6c4f793dc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TetrominoesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_root: str,\n",
    "        tetrominoes_transforms: Callable,\n",
    "        mask_transforms: Callable,\n",
    "        max_n_objects: Optional[int] = 4,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_root = data_root\n",
    "        self.tetrominoes_transforms = tetrominoes_transforms\n",
    "        self.mask_transforms = mask_transforms\n",
    "        self.max_n_objects = max_n_objects\n",
    "        self.split = split\n",
    "        assert os.path.exists(self.data_root), f\"Path {self.data_root} does not exist\"\n",
    "        assert self.split == \"train\" or self.split == \"val\" or self.split == \"test\"\n",
    "\n",
    "        self.data = h5py.File(self.data_root, \"r\")\n",
    "        if self.max_n_objects:\n",
    "            if self.split == \"train\":\n",
    "                num_objects_in_scene = np.sum(self.data[\"visibility\"][:10_000], axis=1)\n",
    "            elif self.split == \"val\":\n",
    "                num_objects_in_scene = np.sum(\n",
    "                    self.data[\"visibility\"][10_001:10_300], axis=1\n",
    "                )\n",
    "            elif self.split == \"test\":\n",
    "                num_objects_in_scene = np.sum(\n",
    "                    self.data[\"visibility\"][75_001:90_000], axis=1\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            num_objects_in_scene -= 1  # remove background\n",
    "            self.indices = (\n",
    "                np.argwhere(num_objects_in_scene <= self.max_n_objects).flatten()\n",
    "                + {\"train\": 0, \"val\": 60_001, \"test\": 75_001}[self.split]\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        if self.max_n_objects:\n",
    "            index_to_load = self.indices[index]\n",
    "        else:\n",
    "            index_to_load = index\n",
    "        img = self.data[\"image\"][index_to_load]\n",
    "        if self.split == \"train\":\n",
    "            return self.tetrominoes_transforms(img)\n",
    "        else:\n",
    "            mask = self.data[\"mask\"][index_to_load]\n",
    "            vis = self.data[\"visibility\"][index_to_load]\n",
    "            return self.tetrominoes_transforms(img), self.mask_transforms(mask), vis\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices if self.max_n_objects else self.data[\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "aea82009-e7de-4281-9cbb-d1a1dc12b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_all_but_last(tensor, n_dims=1):\n",
    "    shape = list(tensor.shape)\n",
    "    batch_dims = shape[:-n_dims]\n",
    "    flat_tensor = torch.reshape(tensor, [np.prod(batch_dims)] + shape[-n_dims:])\n",
    "\n",
    "    def unflatten(other_tensor):\n",
    "        other_shape = list(other_tensor.shape)\n",
    "        return torch.reshape(other_tensor, batch_dims + other_shape[1:])\n",
    "\n",
    "    return flat_tensor, unflatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "992d8b95-542b-4cdf-9281-72430b9289d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_transforms(mask):\n",
    "    # Based on https://github.com/deepmind/deepmind-research/blob/master/iodine/modules/data.py#L115\n",
    "    # `mask` has shape [max_num_entities, height, width, channels]\n",
    "    mask = torch.from_numpy(mask)\n",
    "    mask = torch.permute(mask, [0, 3, 1, 2])\n",
    "    # `mask` has shape [max_num_entities, channels, height, width]\n",
    "    flat_mask, unflatten = flatten_all_but_last(mask, n_dims=3)\n",
    "    resize = transforms.Resize(\n",
    "        (35, 35), interpolation=transforms.InterpolationMode.NEAREST\n",
    "    )\n",
    "    flat_mask = resize.forward(flat_mask)\n",
    "    mask = unflatten(flat_mask)\n",
    "    # `mask` has shape [max_num_entities, channels, height, width]\n",
    "    mask = torch.permute(mask, [0, 2, 3, 1])\n",
    "    # `mask` has shape [max_num_entities, height, width, channels]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "ab0fc688-b378-4577-bb8e-397e9e42ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "1f5a2d5a-ba77-4d1d-ace0-4ddef96870fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x308cfded0>"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_path = \"/Users/kirkswanson/PyTorchPractice/data/tetrominoes.h5\"\n",
    "\n",
    "current_transforms = [\n",
    "                # image has shape (H x W x C)\n",
    "                transforms.ToTensor(),  # rescales to range [0.0, 1.0]\n",
    "                # image has shape (C x H x W)\n",
    "            ]\n",
    "current_transforms.append(\n",
    "                    transforms.Lambda(rescale)\n",
    "                )  # rescale between -1 and 1\n",
    "current_transforms.append(\n",
    "    transforms.Resize(\n",
    "        (35, 35), interpolation=transforms.InterpolationMode.NEAREST\n",
    "    )\n",
    ")\n",
    "tetrominoes_transforms = transforms.Compose(current_transforms)\n",
    "\n",
    "# dataset = TetrominoesDataset(data_path, tetrominoes_transforms=tetrominoes_transforms, mask_transforms=None)\n",
    "\n",
    "train_dataset = TetrominoesDataset(\n",
    "            data_root=data_path,\n",
    "            tetrominoes_transforms=tetrominoes_transforms,\n",
    "            mask_transforms=mask_transforms,\n",
    "            split=\"train\",\n",
    "            max_n_objects=3,\n",
    ")\n",
    "val_dataset = TetrominoesDataset(\n",
    "            data_root=data_path,\n",
    "            tetrominoes_transforms=tetrominoes_transforms,\n",
    "            mask_transforms=mask_transforms,\n",
    "            split=\"val\",\n",
    "            max_n_objects=3,\n",
    ")\n",
    "image_translated = ((train_dataset[0] + 1) / 2) * 255\n",
    "plt.imshow(image_translated.permute(1, 2, 0).int())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "c7599ec1-dd6f-4992-a84d-0962edda385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_grid(resolution):\n",
    "    ranges = [torch.linspace(0., 1., steps=res) for res in resolution]\n",
    "    grid = torch.stack(torch.meshgrid(*ranges, indexing=\"ij\"), dim=-1)\n",
    "    grid = grid.unsqueeze(0)  # Add batch dimension\n",
    "    return torch.cat([grid, 1.0 - grid], dim=-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "0d4459e0-6dc5-4d8f-9315-451273ccedcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftPositionEmbed(nn.Module):\n",
    "    def __init__(self, hidden_size, resolution):\n",
    "        super(SoftPositionEmbed, self).__init__()\n",
    "        self.embedding = nn.Linear(4, hidden_size).to(device)\n",
    "        self.grid = build_grid(resolution)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        grid = self.embedding(self.grid)\n",
    "        return inputs + grid\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "b07247a6-40a2-4e55-9fb6-6017def1918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttention(nn.Module):\n",
    "    def __init__(self, num_slots: int, dim: int, iters: int = 3, epsilon: float = 1e-8, hidden_size: int = 128):\n",
    "        super(SlotAttention, self).__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.iters = iters\n",
    "        self.epsilon = epsilon\n",
    "        self.scale = dim ** -0.5\n",
    "\n",
    "        self.slots_mu = nn.Parameter(torch.randn(1, 1, dim)).to(device)\n",
    "        self.slots_log_sigma = nn.Parameter(torch.randn(1, 1, dim)).to(device)\n",
    "\n",
    "        self.project_q = nn.Linear(dim, dim).to(device)\n",
    "        self.project_k = nn.Linear(dim, dim).to(device)\n",
    "        self.project_v = nn.Linear(dim, dim).to(device)\n",
    "\n",
    "        self.gru = nn.GRUCell(dim, dim).to(device)\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, hidden_size).to(device)\n",
    "        self.fc2 = nn.Linear(hidden_size, dim).to(device)\n",
    "\n",
    "        self.norm_input = nn.LayerNorm(dim).to(device)\n",
    "        self.norm_slots = nn.LayerNorm(dim).to(device)\n",
    "        self.norm_mlp = nn.LayerNorm(dim).to(device)\n",
    "\n",
    "        self.slot_mlp = nn.Sequential(self.norm_mlp, self.fc1, nn.ReLU(), self.fc2).to(device)\n",
    "\n",
    "    def forward(self, inputs: torch.tensor) -> torch.tensor: \n",
    "        B, N, D = inputs.shape\n",
    "\n",
    "        mu = self.slots_mu.expand(B, self.num_slots, -1)\n",
    "        sigma = torch.exp(self.slots_log_sigma.expand(B, self.num_slots, -1))\n",
    "        slots = torch.normal(mu, sigma)\n",
    "\n",
    "        inputs = self.norm_input(inputs)\n",
    "        k, v = self.project_k(inputs), self.project_v(inputs)\n",
    "\n",
    "        for _ in range(self.iters):\n",
    "            slots_prev = slots\n",
    "\n",
    "            slots = self.norm_slots(slots)\n",
    "            q = self.project_q(slots)\n",
    "\n",
    "            attn = torch.einsum('bid,bjd->bij', q, k) * self.scale\n",
    "            attn = attn.softmax(dim=1) + self.epsilon\n",
    "            attn = attn / attn.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            updates = torch.einsum('bjd,bij->bid', v, attn)\n",
    "\n",
    "            slots = self.gru(\n",
    "                updates.reshape(-1, D),\n",
    "                slots_prev.reshape(-1, D),\n",
    "            )\n",
    "\n",
    "            slots = slots.reshape(B, -1, D)\n",
    "            slots = slots + self.slot_mlp(slots)\n",
    "\n",
    "        return slots\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "64b14f83-b1b4-4852-ba4d-04138a2bc135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, resolution, hid_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, hid_dim, 5, padding = 2).to(device)\n",
    "        self.conv2 = nn.Conv2d(hid_dim, hid_dim, 5, padding = 2).to(device)\n",
    "        self.conv3 = nn.Conv2d(hid_dim, hid_dim, 5, padding = 2).to(device)\n",
    "        self.conv4 = nn.Conv2d(hid_dim, hid_dim, 5, padding = 2).to(device)\n",
    "        self.encoder_pos = SoftPositionEmbed(hid_dim, resolution).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = x.permute(0,2,3,1)\n",
    "        x = self.encoder_pos(x)\n",
    "        x = torch.flatten(x, 1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "7f96c2b5-1183-463d-8858-620297bc468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 32, 35, 35])\n",
      "torch.Size([32, 4, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 35, 35, 32).permute(0,3,1,2)\n",
    "print(x.shape)\n",
    "x = nn.ConvTranspose2d(32, 4, 3, stride=(1, 1), padding=1)(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "761100f9-2fc2-4a87-ae94-8001b4471e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hid_dim, resolution):\n",
    "        super().__init__()\n",
    "        # self.conv1 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n",
    "        # self.conv2 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n",
    "        # self.conv3 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n",
    "        # self.conv4 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(2, 2), padding=2, output_padding=1).to(device)\n",
    "        # self.conv5 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(1, 1), padding=2).to(device)\n",
    "        # self.conv6 = nn.ConvTranspose2d(hid_dim, 4, 3, stride=(1, 1), padding=1).to(device)\n",
    "        # self.decoder_initial_size = (8, 8)\n",
    "        self.decoder_initial_size = (35, 35)\n",
    "        self.conv1 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(1, 1), padding=2).to(device)\n",
    "        self.conv2 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(1, 1), padding=2).to(device)\n",
    "        self.conv3 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, stride=(1, 1), padding=2).to(device)\n",
    "        self.conv4 = nn.ConvTranspose2d(hid_dim, 4, 3, stride=(1, 1), padding=1).to(device)\n",
    "        self.decoder_pos = SoftPositionEmbed(hid_dim, self.decoder_initial_size).to(device)\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder_pos(x)\n",
    "        x = x.permute(0,3,1,2)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "#         x = F.pad(x, (4,4,4,4)) # no longer needed\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        # x = self.conv5(x)\n",
    "        # x = F.relu(x)\n",
    "        # x = self.conv6(x)\n",
    "        x = x[:,:,:self.resolution[0], :self.resolution[1]]\n",
    "        x = x.permute(0,2,3,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "43e04020-a424-4271-99f9-8987a8dbdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttentionAutoEncoder(nn.Module):\n",
    "    def __init__(self, resolution, num_slots, num_iterations, hidden_size):\n",
    "        super(SlotAttentionAutoEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.resolution = resolution\n",
    "        self.num_slots = num_slots\n",
    "        self.num_iterations = num_iterations\n",
    "\n",
    "        self.encoder_cnn = Encoder(self.resolution, self.hidden_size)\n",
    "        self.decoder_cnn = Decoder(self.hidden_size, self.resolution)\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.slot_attention = SlotAttention(\n",
    "            num_slots=self.num_slots,\n",
    "            dim=self.hidden_size,\n",
    "            iters = self.num_iterations,\n",
    "            epsilon=1e-8, \n",
    "            hidden_size=128)\n",
    "            \n",
    "    def forward(self, image):\n",
    "        x = self.encoder_cnn(image)\n",
    "        x = nn.LayerNorm(x.shape[1:]).to(device)(x)  #TODO understand this shape input\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        slots = self.slot_attention(x)\n",
    "\n",
    "        slots = slots.reshape((-1, slots.shape[-1])).unsqueeze(1).unsqueeze(2)\n",
    "        slots = slots.repeat((1, 35, 35, 1))\n",
    "\n",
    "        x = self.decoder_cnn(slots)\n",
    "\n",
    "        recons, masks = x.reshape(image.shape[0], -1, x.shape[1], x.shape[2], x.shape[3]).split([3,1], dim=-1)\n",
    "\n",
    "        # Normalize alpha masks over slots.\n",
    "        masks = nn.Softmax(dim=1)(masks)\n",
    "        recon_combined = torch.sum(recons * masks, dim=1)  # Recombine image.\n",
    "        recon_combined = recon_combined.permute(0,3,1,2)\n",
    "        # `recon_combined` has shape: [batch_size, width, height, num_channels].\n",
    "\n",
    "        return recon_combined, recons, masks, slots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "3231bf2b-8d3c-458d-9f66-850561932198",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() and torch.backends.mps.is_built() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2368bee9-24e8-4cc7-b116-d04ccd3258ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:38,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Iterations 156, Train Loss: 0.22137974039713543, Test Loss 0.02258460318788569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:38,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Iterations 312, Train Loss: 0.22141048177083333, Test Loss 0.022615952694669685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:38,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Iterations 468, Train Loss: 0.22137841796875, Test Loss 0.02260932262907637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:38,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Iterations 624, Train Loss: 0.22144034830729167, Test Loss 0.022650368670199778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:38,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Iterations 780, Train Loss: 0.22139310709635418, Test Loss 0.022631244456514397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:38,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Iterations 936, Train Loss: 0.221384765625, Test Loss 0.022629319353306546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:39,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Iterations 1092, Train Loss: 0.2213924357096354, Test Loss 0.022600430123349453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Iterations 1248, Train Loss: 0.2214404093424479, Test Loss 0.022622341805316033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Iterations 1404, Train Loss: 0.22141560872395832, Test Loss 0.022605053921963308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:42,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Iterations 1560, Train Loss: 0.22139290364583333, Test Loss 0.02261278477120907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Iterations 1716, Train Loss: 0.22139896647135418, Test Loss 0.022673599263455007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Iterations 1872, Train Loss: 0.22136421712239585, Test Loss 0.02259595343407164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Iterations 2028, Train Loss: 0.22136824544270833, Test Loss 0.022615592530433166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Iterations 2184, Train Loss: 0.22141725667317708, Test Loss 0.02261187421514633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Iterations 2340, Train Loss: 0.22138800048828125, Test Loss 0.022642518611664466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Iterations 2496, Train Loss: 0.22139479573567708, Test Loss 0.02261855754446476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Iterations 2652, Train Loss: 0.22136494954427083, Test Loss 0.022642739275668528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:43,  3.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Iterations 2808, Train Loss: 0.22148899332682292, Test Loss 0.022634567098414646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Iterations 2964, Train Loss: 0.22146931966145833, Test Loss 0.02263412577040652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Iterations 3120, Train Loss: 0.22140175374348958, Test Loss 0.022663895120012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Iterations 3276, Train Loss: 0.22141377766927084, Test Loss 0.02262067033889446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:44,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Iterations 3432, Train Loss: 0.22137650553385416, Test Loss 0.02263091980142796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [16:04,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Iterations 3588, Train Loss: 0.22135882568359375, Test Loss 0.02263415367045301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:57,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Iterations 3744, Train Loss: 0.2213800048828125, Test Loss 0.022627102567794474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143it [00:35,  4.09it/s]"
     ]
    }
   ],
   "source": [
    "model = SlotAttentionAutoEncoder(resolution=(35, 35), num_slots=4, num_iterations=3, hidden_size=32).to(device)\n",
    "# dataset = ShapeDataset(IMAGE_SIZE, RADIUS, DATASET_SIZE)\n",
    "warmup_steps = 10000\n",
    "learning_rate = 0.0004\n",
    "decay_rate = 0.5\n",
    "decay_steps = 100000\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=64,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "# # Create the DataLoader instance\n",
    "# train_size = int(0.8 * len(dataset))  \n",
    "# test_size = len(dataset) - train_size  \n",
    "\n",
    "# # Split the dataset\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "i = 0\n",
    "for epoch in range(1000):  \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, image in tqdm(enumerate(train_dataloader)):\n",
    "        i += 1\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        if i < warmup_steps:\n",
    "            learning_rate = learning_rate * (i / warmup_steps)\n",
    "        else:\n",
    "            learning_rate = learning_rate\n",
    "\n",
    "        learning_rate = learning_rate * (decay_rate ** (\n",
    "            i / decay_steps))\n",
    "\n",
    "        optimizer.param_groups[0]['lr'] = learning_rate\n",
    "\n",
    "        image = image.to(device)\n",
    "        recon_combined, recons, masks, slots = model(image)\n",
    "        loss = criterion(recon_combined, image)   \n",
    "        train_loss += loss\n",
    "\n",
    "        del recons, masks, slots\n",
    " \n",
    "        loss.backward()                    \n",
    "        optimizer.step()\n",
    "    \n",
    "    test_loss = 0\n",
    "    model.eval() \n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (image, mask, viz) in enumerate(val_dataloader):\n",
    "            image = image.to(device)\n",
    "            recon_combined, recons, masks, slots = model(image)\n",
    "            loss = criterion(recon_combined, image)\n",
    "            test_loss += loss\n",
    "            del recons, masks, slots\n",
    "    print(f\"Epoch {epoch + 1}, Iterations {i}, Train Loss: {train_loss.item() / len(train_loader)}, Test Loss {test_loss.item() / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b853e3-250d-4a41-85fd-97e40cbfa903",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_slots = 4\n",
    "image = train_dataset[0]\n",
    "x = test_image.permute(1, 2, 0).detach().numpy()\n",
    "plt.imshow(x)\n",
    "recon_combined, recons, masks, slots = model(image.unsqueeze(0))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as Image, ImageEnhance\n",
    "fig, ax = plt.subplots(1, 2 + num_slots, figsize=(15, 2))\n",
    "image = image.squeeze(0)\n",
    "recon_combined = recon_combined.squeeze(0)\n",
    "recons = recons.squeeze(0)\n",
    "masks = masks.squeeze(0)\n",
    "image = image.permute(1,2,0).cpu().numpy()\n",
    "recon_combined = recon_combined.permute(1,2,0)\n",
    "recon_combined = recon_combined.cpu().detach().numpy()\n",
    "recons = recons.cpu().detach().numpy()\n",
    "masks = masks.cpu().detach().numpy()\n",
    "ax[0].imshow(image)\n",
    "ax[0].set_title('Image')\n",
    "ax[1].imshow(recon_combined.astype(int))\n",
    "ax[1].set_title('Recon.')\n",
    "for i in range(4):\n",
    "  picture = recons[i] * masks[i] + (1 - masks[i])\n",
    "  ax[i + 2].imshow(picture.astype(int))\n",
    "  ax[i + 2].set_title('Slot %s' % str(i + 1))\n",
    "for i in range(len(ax)):\n",
    "  ax[i].grid(False)\n",
    "  ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c676e9a4-e3b9-466f-a71f-b0e75741237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KIRK torch.Size([32, 80, 80, 4])\n",
      "KIRK torch.Size([16, 2, 80, 80, 3]) torch.Size([16, 2, 80, 80, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([])) must be the same as input size (torch.Size([8]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[249], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(output_logits)):\n\u001b[0;32m---> 25\u001b[0m     loss_v1 \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m criterion(output_logits[i][\u001b[38;5;241m1\u001b[39m], labels[i][\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     26\u001b[0m     loss_v2 \u001b[38;5;241m=\u001b[39m criterion(output_logits[i][\u001b[38;5;241m0\u001b[39m], labels[i][\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m+\u001b[39m criterion(output_logits[i][\u001b[38;5;241m1\u001b[39m], labels[i][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mmin\u001b[39m(loss_v1, loss_v2)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/modules/loss.py:819\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/torch/nn/functional.py:3624\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3625\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3626\u001b[0m     )\n\u001b[1;32m   3628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3629\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3630\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([])) must be the same as input size (torch.Size([8]))"
     ]
    }
   ],
   "source": [
    "model = SlotAttentionAutoEncoder(resolution=(80, 80), num_slots=2, num_iterations=3, hidden_size=64)\n",
    "dataset = ShapeDataset(IMAGE_SIZE, RADIUS, DATASET_SIZE)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create the DataLoader instance\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "test_size = len(dataset) - train_size  # The rest for testing\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "for epoch in range(20):  # Example: 5 epochs\n",
    "# Iterate through the data loader (for example, in a training loop)\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, labels) in tqdm(enumerate(train_loader)):\n",
    "        optimizer.zero_grad()               # Clear gradients\n",
    "        output_logits = model(data)        # Forward pass\n",
    "        # Loss for slot 1 = square, slot 2 = circle\n",
    "        loss = 0\n",
    "        for i in range(len(output_logits)):\n",
    "            loss_v1 = criterion(output_logits[i][0], labels[i][0]) + criterion(output_logits[i][1], labels[i][1])\n",
    "            loss_v2 = criterion(output_logits[i][0], labels[i][1]) + criterion(output_logits[i][1], labels[i][0])\n",
    "            loss = loss + min(loss_v1, loss_v2)\n",
    "        loss.backward()                     # Backward pass\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "    test_loss = 0\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (data, labels) in enumerate(test_loader):\n",
    "            output_logits = model(data)        # Forward pass\n",
    "            for i in range(len(output_logits)):\n",
    "                loss_v1 = criterion(output_logits[i][0], labels[i][0]) + criterion(output_logits[i][1], labels[i][1])\n",
    "                loss_v2 = criterion(output_logits[i][0], labels[i][1]) + criterion(output_logits[i][1], labels[i][0])\n",
    "                test_loss += min(loss_v1, loss_v2)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss.item() / len(train_loader)}, Test Loss {test_loss.item() / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4289b1b1-4e52-4740-8235-9f7d469fe25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..255.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999, 0.0038]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image, test_label = train_dataset[0]\n",
    "x = test_image.permute(1, 2, 0).detach().numpy()\n",
    "plt.imshow(x)\n",
    "test_label\n",
    "output_logits = model(test_image.unsqueeze(0))  # Forward pass\n",
    "print(nn.Sigmoid()(output_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a694873d-dfd2-4d6e-9e45-d7137fb2a92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x151aaa770>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(attn_logits.reshape(20, 20, 2)[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0149d840-441c-450c-95a5-d341497aeed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "3cf30cdd-4224-471d-8be6-d7e1233dcfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image, test_label = test_dataset[1500]\n",
    "test_image = test_image.unsqueeze(0)\n",
    "# model = Model()\n",
    "# output = model(test_image.unsqueeze(0))\n",
    "\n",
    "\n",
    "# Convolutional Layer 1: Input channels = 3 (e.g., color image), Output channels = 16\n",
    "conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "# Convolutional Layer 2: Input channels = 16, Output channels = 32\n",
    "conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "positional_embedding = nn.Parameter(torch.randn(1, 32, 10, 10))\n",
    "\n",
    "# Pooling layer\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "# Slots\n",
    "slots = nn.Parameter(torch.normal(mean=0, std=1, size=(2, 32)))\n",
    "input_layer_norm = nn.LayerNorm(32)\n",
    "slot_layer_norm = nn.LayerNorm(32)\n",
    "slot_mlp_layer = nn.Linear(in_features=32, out_features=32)\n",
    "\n",
    "\n",
    "# Embedding layers\n",
    "k_embed = nn.Linear(in_features=32, out_features=32)\n",
    "v_embed = nn.Linear(in_features=32, out_features=32)\n",
    "q_embed = nn.Linear(in_features=32, out_features=32)\n",
    "gru = nn.GRUCell(32, 32)\n",
    "\n",
    "final_mlp_layer = nn.Linear(in_features=32, out_features=2)\n",
    "\n",
    "inputs = pool(conv2(pool(conv1(test_image)))) + positional_embedding\n",
    "inputs = input_layer_norm(k_embed(inputs.reshape(x.shape[0], -1, inputs.shape[1])))\n",
    "slots = q_embed(slots)\n",
    "slots = slots.unsqueeze(0).repeat(inputs.shape[0], 1, 1)  # [B, S, D]\n",
    "\n",
    "for _ in range(3): \n",
    "    prev_slots = slots  # [B, S, D]\n",
    "    slots = slot_layer_norm(slots).permute(0, 2, 1)  # [B, D, S]\n",
    "    attn = F.softmax(torch.bmm(inputs, slots)*1./math.sqrt(32), dim=-1)\n",
    "    attn = attn / attn.sum(dim=1)\n",
    "    attn = attn.permute(0, 2, 1)\n",
    "    values = v_embed(inputs)\n",
    "    updates = torch.bmm(attn, values)  # [B, S, D]\n",
    "    updated_slots = []\n",
    "    for i in range(updates.shape[1]):\n",
    "        updated_slots.append(gru(updates[:, i, :], prev_slots[:, i, :]).unsqueeze(1))\n",
    "    updated_slots = torch.cat(updated_slots, dim=1)\n",
    "    updated_slots = slot_layer_norm(updated_slots)\n",
    "    slots = slot_mlp_layer(updated_slots)\n",
    "\n",
    "outputs = final_mlp_layer(slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "b980d229-7af7-4343-8564-f12754cba8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 2])"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "c6d85303-e3b9-4dd2-b42f-2ee3c67aff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_slots = []\n",
    "for i in range(updates.shape[1]):\n",
    "    updated_slots.append(gru(updates[:, i, :], prev_slots[:, i, :]).unsqueeze(1))\n",
    "updated_slots = torch.cat(updated_slots, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "7ba07d05-e1de-4855-b03d-327c6f5dab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 32])"
      ]
     },
     "execution_count": 818,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_slots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "6b67d919-ce28-4e91-aeb3-4a02a5a34802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32])"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_slot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b506bf22-7cb2-4030-9564-192356ed912b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
